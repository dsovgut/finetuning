# Fineâ€‘Tuning **Mistralâ€‘7B** with **QLoRA**

> *Genâ€¯AI (CMSCâ€¯37815) â€” UChicago MSâ€‘ADS*

---

## ðŸ“š Overview

This repo contains the notebook and lightweight LoRA adapter I produced for the course assignment:

1. **Objective** â€“ use **QLoRA** (4â€‘bit quantisationÂ + LoRA adapters) to instructionâ€‘tune an openâ€‘weights LLM.
2. **Setup** â€“ base model **`mistralai/Mistralâ€‘7Bâ€‘v0.1`**, trained for **1â€¯epoch** on **1â€¯000** examples from the *Alpaca* instruction dataset.
3. **Deliverables** â€“ notebook (`Finetuning_Danylo.ipynb`).

All hyperâ€‘parameters, code, and evaluation logic follow the template notebook shared in class with only minor modifications (see below).

---

## ðŸ”§ Key Choices

| Component        | Value                                                   | Why                                                          |
| ---------------- | ------------------------------------------------------- | ------------------------------------------------------------ |
| **Base model**   | `mistralai/Mistralâ€‘7Bâ€‘v0.1`                             | Strong reasoning, HFâ€‘native, LoRAâ€‘friendly                   |
| **Dataset**      | `tatsuâ€‘lab/alpaca` â€“ firstÂ 1â€¯000 rows                   | Clean instructionâ€“response pairs; fits course compute budget |
| **Quantisation** | 4â€‘bit GPTQ                                              | Cuts memory footprint dramatically; required for QLoRA       |
| **LoRA config**  | `r=8`, `Î±=16`, dropoutâ€¯0.05, targetâ€¯`q_proj`Â &Â `v_proj` | Standard for 7â€‘B models                                      |
| **Trainer**      | `trl.SFTTrainer`                                        | Matches lecture notebook for reproducibility                 |

---

## ðŸ“ˆ Evaluation

The notebook computes three automatic metrics on a heldâ€‘out 100â€‘example slice:

| Metric          | Score  |
| --------------- | ------ |
| **BLEU**        | 0.5093 |
| **ROUGEâ€‘Lâ€¯F1**  | 0.6298 |
| **ExactÂ Match** | 0.0400 |

*(See the last notebook cell for the raw computation.)*

### Example Generations

<details><summary>Click to expand</summary>

```text
Instruction: List three creative uses for a paper clip.

Model:  
1. Mini phone stand for video calls.  
2. Emergency zipper pull.  
3. Earâ€‘bud cable tidy.
```

```text
Instruction: Explain a hash table to a 12â€‘yearâ€‘old.

Model:  
Imagine millions of numbered mailboxes. A magic math trick tells you exactly which box to drop your note in, so you can find it instantly later.
```

</details>

---

## Notes

* QLoRA lets us fineâ€‘tune large models on commodity GPUs by freezing base weights and training only rankâ€‘reduced adapters on a 4â€‘bit quantised backbone.
* Automatic metrics only approximate quality; manual spotâ€‘checks are essential.
* Feel free to open an issue if you hit a snag reproducing the run.

---

## Acknowledgements
* Course materials by **Prof. Nousetouane** (Genâ€¯AI, Springâ€¯2025).
